{
  "repo_notes": [
    {
      "content": "Autonomous visual servoing system for ROS 2 Jazzy and Gazebo Harmonic."
    }
  ],
  "pages": [
    {
      "title": "Home",
      "purpose": "Welcome page providing an overview of the aruco_visual_servoing system, its purpose, key features, and navigation to other wiki sections",
      "page_notes": [
        {
          "content": "# Welcome to the ArUco Visual Servoing Wiki\n\nThis wiki documents the **aruco_visual_servoing** package, a complete autonomous robotics system developed for **ROS 2 Jazzy** and **Gazebo Harmonic**. The system enables a mobile robot to autonomously explore an environment, detect ArUco markers, sort them by ID, and sequentially visit them using visual servoing techniques.\n\n## ðŸš€ Project Overview\n\n* **Goal:** Autonomous identification and sequential visitation of 5 distinct ArUco markers.\n* **Tech Stack:** ROS 2 Jazzy, Gazebo Harmonic (GZ Sim), Python 3.10+, OpenCV.\n* **Key Capabilities:**\n    * **Perception:** Custom OpenCV-based marker detection.\n    * **Control:** Proportional (P) controller for precise alignment and approach.\n    * **Simulation:** High-fidelity simulation with custom SDF models and URDF robot description.\n\n## ðŸ“š Navigation\n\n* **[Getting Started](Getting-Started):** Installation and first run.\n* **[System Architecture](System-Architecture):** How the nodes and topics interact.\n* **[Simulation Environment](Simulation-Environment):** Details on the robot and world.\n* **[Developer Guide](Developer-Guide):** Deep dive into the code and algorithms."
        }
      ]
    },
    {
      "title": "Getting Started",
      "purpose": "Guide users through installation, setup, and first-time execution of the system",
      "page_notes": [
        {
          "content": "# Getting Started\n\nThis section guides you through setting up your development environment and running the simulation for the first time.\n\n### Prerequisites\n\n* **OS:** Ubuntu 24.04 (Noble Numbat) recommended.\n* **ROS Version:** ROS 2 Jazzy Jalisco.\n* **Simulator:** Gazebo Harmonic.\n\nEnsure your system is up to date before proceeding with the [Installation](Installation-and-Dependencies)."
        }
      ]
    },
    {
      "title": "Installation and Dependencies",
      "purpose": "Detailed instructions for installing ROS 2 Jazzy, Gazebo Harmonic, and all required dependencies",
      "parent": "Getting Started",
      "page_notes": [
        {
          "content": "# Installation\n\n## 1. Clone the Repository\nNavigate to your ROS 2 workspace `src` directory:\n```bash\ncd ~/ros2_ws/src\ngit clone [https://github.com/mohamedeyaad/aruco_visual_servoing.git](https://github.com/mohamedeyaad/aruco_visual_servoing.git)\n```\n\n## 2. Install Dependencies\nInstall the required Python libraries for computer vision and ROS-Gazebo bridging:\n```bash\nsudo apt install ros-jazzy-ros-gz-bridge ros-jazzy-ros-gz-sim ros-jazzy-cv-bridge python3-opencv\n```\n\n## 3. Build the Workspace\nThe repository contains two packages: `aruco_visual_servoing` and `aruco_interfaces`. Build them using colcon:\n```bash\ncd ~/ros2_ws\ncolcon build --symlink-install\nsource install/setup.bash\n```\n\n## 4. Environment Setup (Critical)\nFor Gazebo to locate the custom ArUco marker models, you must export the model path. Add this to your `~/.bashrc` or run it in your terminal:\n```bash\nexport GZ_SIM_RESOURCE_PATH=$GZ_SIM_RESOURCE_PATH:$HOME/ros2_ws/src/aruco_visual_servoing/aruco_visual_servoing/models\n```"
        }
      ]
    },
    {
      "title": "Running the Simulation",
      "purpose": "Step-by-step guide to launching the Gazebo simulation and visual servoing nodes",
      "parent": "Getting Started",
      "page_notes": [
        {
          "content": "# Running the System\n\nThe system is launched in two stages: the simulation environment and the control software.\n\n## Step 1: Launch Simulation\nThis command opens Gazebo Harmonic, spawns the robot, and starts the ROS-GZ bridge.\n```bash\nros2 launch aruco_visual_servoing simulation.launch.py\n```\n*Wait for the simulation to fully load before proceeding.*\n\n## Step 2: Start Control Logic\nThis launches the detector node and the visual servoing controller.\n```bash\nros2 launch aruco_visual_servoing servoing.launch.py\n```\n\n## Optional: Skid-Steer Variant\nTo run the simulation with the 4-wheel skid-steer robot:\n```bash\ngit checkout skid-steer\ncolcon build\nros2 launch aruco_visual_servoing simulation.launch.py\n```"
        }
      ]
    },
    {
      "title": "Understanding System Behavior",
      "purpose": "Explain the expected behavior when the system runs, including the FSM states and marker visitation sequence",
      "parent": "Getting Started",
      "page_notes": [
        {
          "content": "# Expected Behavior\n\nOnce both launch files are running, the robot will execute the following autonomous sequence:\n\n1.  **Scanning (SEARCHING):** The robot rotates in place to scan the environment. It builds a list of all unique ArUco marker IDs it detects.\n2.  **Sorting:** Once the scan is complete (all 5 markers found), the system sorts the IDs in ascending order (e.g., 0, 1, 2, 3, 4).\n3.  **Chasing (ALIGNING):** The robot identifies the first target in the list. It rotates to center the marker in its camera frame and moves forward until it is exactly **1.0 meter** away.\n4.  **Visiting (VISITING):** Upon reaching the target, the robot stops for a set duration to \"visit\" the marker. A green circle is drawn around the target in the debug view.\n5.  **Loop:** The robot proceeds to the next ID in the sorted list and repeats the Align/Visit process until all markers have been visited."
        }
      ]
    },
    {
      "title": "System Architecture",
      "purpose": "High-level architectural overview of the entire system, including major components and their interactions",
      "page_notes": [
        {
          "content": "# System Architecture\n\nThe `aruco_visual_servoing` system follows a modular ROS 2 architecture, separating perception, control, and simulation.\n\n\n\n## Core Interaction Loop\n1.  **Gazebo** simulates the physics and camera sensor.\n2.  **ros_gz_bridge** translates Gazebo images to ROS 2 topics.\n3.  **aruco_detector_node** processes images to find marker poses.\n4.  **visual_servoing_node** calculates velocity commands based on marker poses.\n5.  **ros_gz_bridge** sends velocity commands back to the simulated robot."
        }
      ]
    },
    {
      "title": "Component Overview",
      "purpose": "Describe the major subsystems: simulation, perception, control, and visualization layers",
      "parent": "System Architecture",
      "page_notes": [
        {
          "content": "# Component Layers\n\n### 1. Simulation Layer\n* **Gazebo Harmonic:** Provides the physics engine and rendering.\n* **World:** Custom environment with 5 ArUco markers mapped onto box geometries.\n* **Robot:** Differential drive robot with a simulated camera sensor.\n\n### 2. Perception Layer\n* **OpenCV:** Used for image processing and marker detection.\n* **cv_bridge:** Converts ROS image messages to OpenCV format.\n\n### 3. Control Layer\n* **Finite State Machine:** Manages the high-level mission logic.\n* **P-Controller:** Handles low-level velocity generation for alignment and approach.\n\n### 4. Visualization Layer\n* **RViz2:** Visualizes the robot model, camera feed, and debug overlays.\n* **Debug Topics:** Publishes processed images highlighting the active target."
        }
      ]
    },
    {
      "title": "ROS 2 Package Structure",
      "purpose": "Explain the two-package architecture (aruco_interfaces and aruco_visual_servoing) and their purposes",
      "parent": "System Architecture",
      "page_notes": [
        {
          "content": "# Package Structure\n\nThe project is split into two packages to ensure clean dependency management:\n\n### `aruco_visual_servoing`\nThe main application package containing:\n* Python source code (nodes)\n* Launch files\n* Configuration (YAML)\n* Simulation assets (URDF, SDF models, worlds)\n\n### `aruco_interfaces`\nA dedicated interface package containing:\n* Custom message definitions (`.msg`)\n* Ensures that message types can be shared easily between nodes without circular dependencies."
        }
      ]
    },
    {
      "title": "Communication and Data Flow",
      "purpose": "Detail the ROS 2 topics, message types, and data flow between nodes",
      "parent": "System Architecture",
      "page_notes": [
        {
          "content": "# Data Flow\n\n| Topic | Type | Source | Destination | Description |\n| :--- | :--- | :--- | :--- | :--- |\n| `/camera/image_raw` | `sensor_msgs/Image` | ros_gz_bridge | Detector Node | Raw video feed from simulation |\n| `/aruco_markers` | `aruco_interfaces/ArucoMarkers` | Detector Node | Servoing Node | List of detected IDs and their poses |\n| `/cmd_vel` | `geometry_msgs/Twist` | Servoing Node | ros_gz_bridge | Velocity commands for the robot |\n| `/aruco_target_circled` | `sensor_msgs/Image` | Servoing Node | RViz2 | Debug image with target highlighted |\n| `/joint_states` | `sensor_msgs/JointState` | ros_gz_bridge | robot_state_publisher | Robot wheel positions for TF |"
        }
      ]
    },
    {
      "title": "Finite State Machine",
      "purpose": "Deep dive into the FSM controlling autonomous behavior: SEARCHING, ALIGNING, VISITING, COMPLETED states",
      "parent": "System Architecture",
      "page_notes": [
        {
          "content": "# Finite State Machine (FSM)\n\nThe `visual_servoing_node` operates on a strict FSM to manage mission logic.\n\n### 1. SEARCHING\n* **Trigger:** System start.\n* **Action:** Rotate robot at fixed speed (`0.4 rad/s`).\n* **Transition:** When `len(found_ids) >= 5` (all markers found).\n\n### 2. ALIGNING\n* **Trigger:** Target selected from sorted list.\n* **Action:** \n    * Rotate to minimize horizontal error (`err_x`).\n    * Move forward to minimize distance error (`dist`).\n* **Transition:** When `dist <= 1.0m` and `abs(err_x) < 0.05`.\n\n### 3. VISITING\n* **Trigger:** Target reached.\n* **Action:** Stop motors. Publish debug image with green circle. Wait for `visit_duration`.\n* **Transition:** After timer expires, select next ID. If no IDs remain -> COMPLETED.\n\n### 4. COMPLETED\n* **Trigger:** All markers visited.\n* **Action:** Stop motors permanently."
        }
      ]
    },
    {
      "title": "ROS 2 Nodes",
      "purpose": "Detailed documentation of each ROS 2 node in the system",
      "page_notes": [
        {
          "content": "# ROS 2 Nodes\n\nThis section details the internal logic and interfaces of the custom Python nodes developed for this package."
        }
      ]
    },
    {
      "title": "ArUco Detector Node",
      "purpose": "Complete reference for aruco_detector_node: parameters, subscribed topics, published topics, and internal operation",
      "parent": "ROS 2 Nodes",
      "page_notes": [
        {
          "content": "# ArUco Detector Node\n\n**File:** `aruco_detector_node.py`\n\nThis node acts as the vision system. It bridges ROS 2 and OpenCV to detect markers in the raw camera stream.\n\n### Subscriptions\n* `/camera/image_raw` (sensor_msgs/Image): The simulated camera feed.\n\n### Publications\n* `/aruco_markers` (aruco_interfaces/ArucoMarkers): Custom message containing a list of detected `marker_ids` and their corresponding `poses` relative to the camera frame.\n\n### Logic\n1.  Converts ROS image to OpenCV format using `cv_bridge`.\n2.  Applies `cv2.aruco.detectMarkers` using the `DICT_5X5_250` dictionary.\n3.  Estimates pose (rvec, tvec) for each detected marker.\n4.  Publishes the aggregated detection data."
        }
      ]
    },
    {
      "title": "Visual Servoing Controller Node",
      "purpose": "Complete reference for visual_servoing_node: FSM implementation, control algorithm, parameters, and topics",
      "parent": "ROS 2 Nodes",
      "page_notes": [
        {
          "content": "# Visual Servoing Controller Node\n\n**File:** `visual_servoing_node.py`\n\nThe brain of the operation. It consumes detection data and produces velocity commands.\n\n### Parameters\n* `center_kp` (0.4): Proportional gain for angular alignment.\n* `distance_kp` (0.25): Proportional gain for linear approach.\n* `target_dist` (1.0): Desired stopping distance (meters).\n\n### Subscriptions\n* `/aruco_markers`: To know where the targets are.\n* `/camera/image_raw`: Used solely for generating the debug image overlay.\n\n### Publications\n* `/cmd_vel`: To drive the robot.\n* `/aruco_target_circled`: Debug image showing the \"locked-on\" target."
        }
      ]
    },
    {
      "title": "No-Forward Variant",
      "purpose": "Document the alternative visual_servoing_node_no_forward implementation and how it differs from the standard controller",
      "parent": "ROS 2 Nodes",
      "page_notes": [
        {
          "content": "# No-Forward Variant\n\n**File:** `visual_servoing_node_no_forward.py`\n\nA specialized version of the controller used for testing or specific scenarios where linear motion is restricted.\n\n### Differences\n* **Control Logic:** This node only applies **Angular Control**.\n* **Behavior:** It will align the robot's heading with the target marker but will **not** move closer to it.\n* **Use Case:** Ideal for calibrating the `center_kp` gain or testing detection stability without the robot physically moving out of position."
        }
      ]
    },
    {
      "title": "Simulation Environment",
      "purpose": "Documentation of the Gazebo simulation setup, robot model, and world configuration",
      "page_notes": [
        {
          "content": "# Simulation Environment\n\nThe simulation is built on **Gazebo Harmonic** (gz sim) and uses standard URDF/SDF modeling formats."
        }
      ]
    },
    {
      "title": "Robot Model (URDF)",
      "purpose": "Detailed description of the aruco_bot differential drive robot: links, joints, geometry, mass properties",
      "parent": "Simulation Environment",
      "page_notes": [
        {
          "content": "# Robot Model (URDF)\n\n**File:** `urdf/aruco_bot_diff.xacro` [cite: 2]\n\nA custom differential drive robot designed for this task.\n\n### Physical Properties\n* **Chassis:** 2.0m x 1.0m x 0.5m box. Mass: 5.0kg.\n* **Wheels:** Cylinder (Radius: 0.4m, Width: 0.2m). Mass: 2.0kg each.\n* **Caster:** Sphere (Radius: 0.2m) for stability. [cite_start]Mass: 1.0kg.\n\n### Sensors\n* **Camera:** Mounted at `(0.9, 0, 0.36)` relative to the base [cite: 11][cite_start].\n* **FOV:** ~80 degrees (1.396 rad)[cite: 43].\n* **Update Rate:** 20 Hz."
        }
      ]
    },
    {
      "title": "Gazebo Plugins and Configuration",
      "purpose": "Explain Gazebo-specific configuration: DiffDrive plugin, camera sensor, joint state publisher, physics parameters",
      "parent": "Simulation Environment",
      "page_notes": [
        {
          "content": "# Gazebo Plugins\n\n**File:** `urdf/aruco_bot_diff.gazebo.xacro` [cite: 38]\n\nThe simulation logic is injected via Gazebo plugins:\n\n1.  **DiffDrive System (`gz-sim-diff-drive-system`):**\n    * Controls the `left_wheel_joint` and `right_wheel_joint`.\n    * Subscribes to `/cmd_vel`.\n    * Publishes odometry to `odom`.\n\n2.  **Sensors System (`gz-sim-sensors-system`):**\n    * Manages the camera data generation.\n    * Publishes to `camera/image` and `camera/camera_info`.\n\n3.  **Joint State Publisher:**\n    * Publishes the state of moving joints to `/joint_states` for TF tree generation."
        }
      ]
    },
    {
      "title": "World and ArUco Markers",
      "purpose": "Document the aruco_world.world file, marker placement, and the marker generation utility",
      "parent": "Simulation Environment",
      "page_notes": [
        {
          "content": "# Simulation World\n\n**File:** `worlds/aruco_world.world` [cite: 12]\n\nA custom SDF world featuring a white ground plane and a specific arrangement of markers.\n\n### Marker Layout\nThe markers are mounted on white cubes and arranged in a circle around the robot spawn point:\n* **Marker 0:** Position (7.0, 0, 1.0)\n* **Marker 1:** Position (2.47, 7.60, 2.0)\n* **Marker 2:** Position (-6.47, 4.70, 2.0)\n* **Marker 3:** Position (-6.47, -4.70, 2.0)\n* **Marker 4:** Position (2.47, -7.60, 2.0)\n\n### Custom Models\nThe markers are implemented as custom SDF models (`aruco_marker_x`) stored in the `models/` directory. Each model applies a generated png texture to a flat plane geometry."
        }
      ]
    },
    {
      "title": "Launch System",
      "purpose": "Explain simulation.launch.py and servoing.launch.py, their roles, and how they orchestrate the system",
      "parent": "Simulation Environment",
      "page_notes": [
        {
          "content": "# Launch System\n\n### `simulation.launch.py`\nOrchestrates the environment startup:\n1.  **Robot State Publisher:** Processes the URDF/Xacro.\n2.  **Gazebo:** Launches `gz_sim` with the `aruco_world.world`.\n3.  **Spawn:** Uses `ros_gz_sim create` to spawn the robot at `(0,0,0.5)`.\n4.  **Bridge:** Starts the `ros_gz_bridge` with the config file.\n5.  **RViz:** Launches RViz2 with a pre-configured view.\n\n### `servoing.launch.py`\nStarts the application logic:\n1.  Launches `aruco_detector_node`.\n2.  Launches `visual_servoing_node`."
        }
      ]
    },
    {
      "title": "ROS-Gazebo Bridge",
      "purpose": "Detail the ros_gz_bridge configuration and topic mappings between ROS 2 and Gazebo",
      "parent": "Simulation Environment",
      "page_notes": [
        {
          "content": "# ROS-Gazebo Bridge\n\n**File:** `config/ros_gz_bridge.yaml`\n\nThe bridge maps Gazebo transport topics (GZ) to ROS 2 topics. Crucial mappings include:\n\n* **Clock:** `/clock` (GZ -> ROS)\n* **Joints:** `/joint_states` (GZ -> ROS)\n* **Velocity:** `/cmd_vel` (ROS -> GZ)\n* **Camera:** `camera/image` -> `camera/image_raw` (GZ -> ROS)"
        }
      ]
    },
    {
      "title": "Message Interfaces",
      "purpose": "Documentation of custom ROS 2 message definitions in the aruco_interfaces package",
      "page_notes": [
        {
          "content": "# Message Interfaces\n\nDetailed documentation for the custom data structures used in this project."
        }
      ]
    },
    {
      "title": "aruco_interfaces Package",
      "purpose": "Explain the purpose and build system of the aruco_interfaces package",
      "parent": "Message Interfaces",
      "page_notes": [
        {
          "content": "# aruco_interfaces\n\nThis is a lightweight package dedicated solely to defining ROS 2 messages. Isolating messages prevents circular build dependencies and allows other packages to subscribe to the detection data without needing the full simulation stack.\n\n**Build Type:** `ament_cmake`"
        }
      ]
    },
    {
      "title": "ArucoMarkers Message Definition",
      "purpose": "Complete specification of the ArucoMarkers.msg custom message: fields, data types, and usage patterns",
      "parent": "Message Interfaces",
      "page_notes": [
        {
          "content": "# ArucoMarkers.msg\n\n**Location:** `aruco_interfaces/msg/ArucoMarkers.msg`\n\nThis message carries the full state of detections for a single video frame.\n\n```text\nstd_msgs/Header header          # Timestamp and Frame ID\n\nint64[] marker_ids              # List of detected IDs (e.g., [0, 4])\ngeometry_msgs/Pose[] poses      # Corresponding 6D poses (x,y,z, orientation)\n```\n\n**Usage:**\n* `marker_ids[i]` corresponds to the pose at `poses[i]`."
        }
      ]
    },
    {
      "title": "Configuration and Visualization",
      "purpose": "Guide to configuring system parameters and using visualization tools",
      "page_notes": [
        {
          "content": "# Configuration\n\nHow to tweak the system and visualize the results."
        }
      ]
    },
    {
      "title": "Parameters",
      "purpose": "Document all configurable parameters: marker size, dictionary, control gains, target distance, etc.",
      "parent": "Configuration and Visualization",
      "page_notes": [
        {
          "content": "# Control Parameters\n\nThe following parameters are defined in `visual_servoing_node.py` and can be modified to tune performance:\n\n| Parameter | Default | Description |\n| :--- | :--- | :--- |\n| `rotation_speed` | 0.4 | Angular velocity (rad/s) during the search phase. |\n| `center_kp` | 0.4 | Proportional gain for centering the robot on the marker. |\n| `distance_kp` | 0.25 | Proportional gain for linear approach velocity. |\n| `target_dist` | 1.0 | The target stop distance (meters) from the marker. |\n| `visit_duration` | 3.0 | Time (seconds) to pause at a marker before moving on. |"
        }
      ]
    },
    {
      "title": "RViz Visualization",
      "purpose": "Explain the RViz configuration, display panels, and how to use it for monitoring and debugging",
      "parent": "Configuration and Visualization",
      "page_notes": [
        {
          "content": "# RViz Visualization\n\n**File:** `rviz/aruco_visual_servoing.rviz`\n\nThe project includes a pre-configured RViz session launched automatically.\n\n### Key Displays\n1.  **RobotModel:** Shows the live URDF state of the robot.\n2.  **Image (Raw):** Displays the raw feed from `/camera/image_raw`.\n3.  **Image (Debug):** Displays `/aruco_target_circled`. This is the most important view for debugging, as it shows which marker the robot is currently targeting (circled in green)."
        }
      ]
    },
    {
      "title": "Developer Guide",
      "purpose": "Resources for developers who want to modify or extend the system",
      "page_notes": [
        {
          "content": "# Developer Guide\n\nTechnical details for contributors and developers."
        }
      ]
    },
    {
      "title": "Package Structure and Entry Points",
      "purpose": "Explain the Python package structure, setup.py, and console script entry points",
      "parent": "Developer Guide",
      "page_notes": [
        {
          "content": "# Package Structure\n\n```text\naruco_visual_servoing/\nâ”œâ”€â”€ aruco_visual_servoing/        # Source code\nâ”‚   â”œâ”€â”€ aruco_detector_node.py\nâ”‚   â”œâ”€â”€ visual_servoing_node.py\nâ”‚   â””â”€â”€ ...\nâ”œâ”€â”€ config/                       # Parameters & Bridge config\nâ”œâ”€â”€ launch/                       # Launch files\nâ”œâ”€â”€ models/                       # SDF Marker models\nâ”œâ”€â”€ urdf/                         # Robot description\nâ””â”€â”€ worlds/                       # Gazebo world\n```\n\n### Entry Points (`setup.py`)\n* `chaser`: Maps to `visual_servoing_node:main`.\n* `detector`: Maps to `aruco_detector_node:main`."
        }
      ]
    },
    {
      "title": "ArUco Marker Generation",
      "purpose": "Tutorial on using aruco_generate_markers.py to create custom marker images",
      "parent": "Developer Guide",
      "page_notes": [
        {
          "content": "# Generating Markers\n\nTo add new markers to the system, you can use the provided utility script.\n\n**Script:** `aruco_generate_markers.py`\n\n### Usage\n```bash\ncd src/aruco_visual_servoing/aruco_visual_servoing\npython3 aruco_generate_markers.py\n```\n\n### Logic\nIt uses `cv2.aruco` to generate 250x250 pixel images from the `DICT_5X5_250` dictionary. Modify the loop range in the script to generate IDs beyond the default 0-4."
        }
      ]
    },
    {
      "title": "Control Algorithm Details",
      "purpose": "Deep dive into the proportional control implementation, error calculations, and tuning guidelines",
      "parent": "Developer Guide",
      "page_notes": [
        {
          "content": "# Control Algorithm\n\nThe visual servoing uses a decoupled Proportional (P) controller.\n\n### 1. Angular Control (Centering)\n* **Input:** `err_x` (Horizontal offset of marker centroid from image center).\n* **Output:** `angular.z = -1.0 * err_x * center_kp`.\n* **Goal:** Keep `err_x` close to 0.\n\n### 2. Linear Control (Approach)\n* **Input:** `dist` (Z-distance from camera to marker).\n* **Output:** `linear.x = (dist - target_dist) * distance_kp`.\n* **Goal:** Stop when `dist == 1.0` meters.\n\n### Deadband\nTo prevent oscillation, the robot considers the alignment complete only when `abs(err_x) < 0.05`."
        }
      ]
    }
  ]
}
